# æ•°æ®å¤„ç†ç®¡é“

> æ³•å¾‹æ–‡æ¡£åˆ†å—ã€å‘é‡åŒ–ä¸å…¥åº“

---

## ğŸ“„ æ–‡æ¡£æ ¼å¼è½¬æ¢

### Pandoc è½¬æ¢

```bash
# PDF â†’ Markdown
pandoc input.pdf -o output.md --extract-media=./media --wrap=none

# DOCX â†’ Markdown
pandoc input.docx -o output.md -t markdown --wrap=none
```

---

## ğŸ”ª ç»“æ„åŒ–åˆ†å—

### æ ¸å¿ƒå®ç°

```python
import re
from typing import Dict, List

def extract_hierarchy(markdown_text: str) -> Dict:
    """æå–æ³•å¾‹å±‚çº§ç»“æ„"""
    hierarchy = {'parts': [], 'chapters': [], 'sections': []}

    lines = markdown_text.split('\n')
    for line in lines:
        if match := re.match(r'^#\s+ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+ç¼–\s+(.+)', line):
            hierarchy['parts'].append(match.group(0))
        elif match := re.match(r'^##\s+ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+ç« \s+(.+)', line):
            hierarchy['chapters'].append(match.group(0))

    return hierarchy

def split_by_articles(markdown_text: str) -> List[Dict]:
    """æŒ‰æ³•æ¡åˆ†å—"""
    articles = []
    current_hierarchy = {'part': None, 'chapter': None, 'section': None}
    current_article = None
    current_lines = []

    for line in markdown_text.split('\n'):
        # æ›´æ–°å±‚çº§
        if re.match(r'^#\s+ç¬¬.+ç¼–', line):
            current_hierarchy['part'] = line.strip('#').strip()
        elif re.match(r'^##\s+ç¬¬.+ç« ', line):
            current_hierarchy['chapter'] = line.strip('#').strip()

        # æ£€æµ‹æ³•æ¡
        if match := re.match(r'^(ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡)\s+(.+)', line):
            if current_article:
                articles.append({
                    'article_number': current_article,
                    'content': '\n'.join(current_lines),
                    'hierarchy': current_hierarchy.copy()
                })
            current_article = match.group(1)
            current_lines = [line]
        elif current_article:
            current_lines.append(line)

    return articles

def extract_references(content: str) -> Dict:
    """æå–å¼•ç”¨å…³ç³»"""
    pattern = re.compile(r'(æœ¬æ³•|ä¾ç…§)?ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡')
    refs = pattern.findall(content)

    return {
        'direct_refs': list(set(refs)),
        'referenced_by': [],
        'related': []
    }

def chunk_legal_document(markdown_text: str, law_name: str, source_file: str) -> List[Dict]:
    """å®Œæ•´åˆ†å—æµç¨‹"""
    articles = split_by_articles(markdown_text)

    chunks = []
    for i, article in enumerate(articles):
        refs = extract_references(article['content'])

        hierarchy_path = ' > '.join(filter(None, [
            article['hierarchy']['part'],
            article['hierarchy']['chapter'],
            article['article_number']
        ]))

        chunks.append({
            'chunk_id': f"{source_file}_{i:04d}",
            'chunk_text': article['content'],
            'chunk_order': i,
            'source_file': source_file,
            'law_name': law_name,
            'law_hierarchy': hierarchy_path,
            'article_number': article['article_number'],
            'references': refs,
        })

    return chunks
```

---

## ğŸŒ² Merkle Tree æ„å»º

```python
from merkletools import MerkleTools
import hashlib

def build_merkle_tree(chunks):
    """æ„å»º Merkle Tree"""
    mt = MerkleTools(hash_type="sha256")

    # è®¡ç®—å¶å­èŠ‚ç‚¹å“ˆå¸Œ
    for chunk in chunks:
        chunk_hash = hashlib.sha256(chunk['chunk_text'].encode('utf-8')).hexdigest()
        chunk['chunk_hash'] = chunk_hash
        mt.add_leaf(chunk_hash, do_hash=False)

    # æ„å»ºæ ‘
    mt.make_tree()
    merkle_root = mt.get_merkle_root()

    # ç”Ÿæˆ Merkle Proof
    for i, chunk in enumerate(chunks):
        proof = mt.get_proof(i)
        chunk['merkle_index'] = i
        chunk['merkle_proof'] = {
            'proof': [p['right'] if p['right'] else p['left'] for p in proof],
            'index': i
        }

    return merkle_root, chunks
```

---

## ğŸ“Š å‘é‡åŒ–

```python
from openai import OpenAI
import os

client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

def generate_embeddings(chunks):
    """æ‰¹é‡ç”Ÿæˆ Embeddings"""
    texts = [chunk['chunk_text'] for chunk in chunks]

    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=texts
    )

    for i, data in enumerate(response.data):
        chunks[i]['embedding'] = data.embedding

    return chunks
```

---

## ğŸ”„ å®Œæ•´æµç¨‹

```python
import json
import os

def process_legal_document(pdf_path: str, law_name: str):
    """ç«¯åˆ°ç«¯å¤„ç†æµç¨‹"""

    # 1. è½¬æ¢ä¸º Markdown
    os.system(f"pandoc {pdf_path} -o temp.md --wrap=none")
    with open('temp.md', 'r', encoding='utf-8') as f:
        markdown_text = f.read()

    # 2. ç»“æ„åŒ–åˆ†å—
    chunks = chunk_legal_document(
        markdown_text=markdown_text,
        law_name=law_name,
        source_file=pdf_path.replace('.pdf', '')
    )

    # 3. æ„å»º Merkle Tree
    merkle_root, chunks = build_merkle_tree(chunks)

    # 4. å‘é‡åŒ–
    chunks = generate_embeddings(chunks)

    # 5. ä¸Šä¼ åˆ°åŒºå—é“¾
    from blockchain_client import publish_version_to_blockchain
    tx_receipt = publish_version_to_blockchain(
        merkle_root=bytes.fromhex(merkle_root),
        description=f"{law_name} v1.0",
        chunk_count=len(chunks)
    )

    # 6. å­˜å…¥ Milvus
    from milvus_client import ingest_to_milvus
    ingest_to_milvus(chunks, version_id=tx_receipt['version_id'])

    print(f"âœ… å¤„ç†å®Œæˆ: {len(chunks)} ä¸ªæ³•æ¡")
    print(f"ğŸ“¦ Merkle Root: {merkle_root}")
    print(f"ğŸ”— TX Hash: {tx_receipt['tx_hash']}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    process_legal_document(
        pdf_path="./data/æ°‘æ³•å…¸.pdf",
        law_name="ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸"
    )
```

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [â† å‘é‡æ•°æ®åº“é…ç½®](./03_vector_database.md)
- [åç«¯å¼€å‘æŒ‡å— â†’](./05_backend_development.md)
